{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0jsZ94-pELX"
      },
      "source": [
        "# ISIC 2024 Challenge: Skin Cancer Detection\n",
        "\n",
        "## Team Contributions\n",
        "\n",
        "### Preprocessing and Exploratory Data Analysis (EDA)\n",
        "- **Contributor:** Debankita Basu\n",
        "- **Responsibilities:** Data cleaning, handling missing values, encoding categorical variables, normalizing continuous variables, and performing exploratory data analysis including visualizations.\n",
        "\n",
        "### Feature Engineering\n",
        "- **Contributor:** Rashi Tiwary\n",
        "- **Responsibilities:** Creating interaction features, addressing class imbalance, dimensionality reduction, feature selection, and transformation.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx_9NzLTSJnw"
      },
      "source": [
        "# 1. Installations and Set-up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ZXzNCrR5S9",
        "outputId": "8a774dde-2b88-4cfb-8697-7448bdbdbde7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.8)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.1.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing required libraries\n",
        "!pip install kaggle\n",
        "!pip install seaborn\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja26Bg3ZSeF_"
      },
      "source": [
        "Setting up Kaggle API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD4t7euuSvXN"
      },
      "source": [
        "To access Kaggle datasets directly from Colab,  set up the Kaggle API.\n",
        "\n",
        "Go to Kaggle account, navigate to Account, and scroll down to the API section.\n",
        "Click Create New API Token. This will download a kaggle.json file containing  API credentials.\n",
        "Now, upload this kaggle.json file to  Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "004W5cC3Sjmb",
        "outputId": "7deddda7-3766-4fb3-bc05-d19efc6abc3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c9f2accc-e940-495f-af54-f9bfbbbb68ce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c9f2accc-e940-495f-af54-f9bfbbbb68ce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-baf3789fce29>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# upload the kaggle.json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # upload the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haS0RqjvTbvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9092d2f8-0a6a-41ac-e00e-52de2165fbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle (2).json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Configure Kaggle API\n",
        "# Once the kaggle.json file is uploaded, move it to the appropriate directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv \"kaggle (2).json\" ~/.kaggle/kaggle.json  # Rename and move in one step\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "spGaiykeTn0O",
        "outputId": "a5d43352-976b-4156-8236-a2bf3dc85d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "unzip:  cannot find or open isic-2024-challenge.zip, isic-2024-challenge.zip.zip or isic-2024-challenge.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Download the ISIC 2024 Dataset using Kaggle API\n",
        "!kaggle competitions download -c isic-2024-challenge\n",
        "!unzip isic-2024-challenge.zip -d isic2024/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzbfRjk3T6An"
      },
      "source": [
        "# 2. Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "id": "cSyXpyhgaxVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kcYdCwvUxv1"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import PolynomialFeatures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa-HGh64VWB0"
      },
      "outputs": [],
      "source": [
        "# Load the training metadata into a Pandas DataFrame to begin EDA\n",
        "train_metadata = pd.read_csv('isic2024/train-metadata.csv')\n",
        "train_metadata.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHvh2tGaTxP"
      },
      "source": [
        "### **Initial Observations:**\n",
        "Missing Data: Several fields contain missing values (NaN). This might require data imputation strategies or careful consideration during feature selection.\n",
        "\n",
        "Class Imbalance: Given that the example rows are all benign, need to find whether there is class imbalance in the full dataset. This is common in medical datasets and might require special handling, such as resampling or using weighted loss functions.\n",
        "\n",
        "Metadata Richness: The dataset is rich in metadata, with 55 columns that can provide valuable insights into each case. Proper feature engineering and selection will be crucial to leveraging this information effectively.\n",
        "\n",
        "Data Consistency: In next steps ensuring that all data types are consistent (e.g., numerical fields should be numeric, categorical fields should be correctly encoded) is necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbU5mn7jpb7"
      },
      "outputs": [],
      "source": [
        "# Display the column names in the train_metadata DataFrame\n",
        "print(train_metadata.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zcbb0BphV0J4"
      },
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "train_metadata.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7v3z8Uebtpn"
      },
      "source": [
        "###**Observations on Missing Values:**\n",
        "\n",
        "age_approx: Approximately 2,798 entries are missing for the patient's approximate age. This is a critical feature, and missing values may need to be imputed or handled carefully.\n",
        "\n",
        "sex: A significant number of entries (11,517) are missing for the patient's sex. This could be important for the analysis, as sex can influence skin cancer risk.\n",
        "\n",
        "anatom_site_general: 5,756 entries lack information on the general location of the lesion on the body. This feature might be important for determining lesion risk based on body location.\n",
        "\n",
        "lesion_id: A very large portion of entries (379,001) are missing the lesion identifier. This column seems to be sparsely populated and might not be useful unless specifically analyzing subsets of the data.\n",
        "\n",
        "iddx_2 to iddx_5 (Lesion Diagnosis Levels): There is a high rate of missing data in these columns, especially for iddx_3 through iddx_5, with nearly 400,000 entries missing. These columns might be highly specific or applicable only to certain cases.\n",
        "\n",
        "mel_mitotic_index and mel_thick_mm: Over 400,000 entries are missing in these melanoma-specific features, indicating they are only relevant for a subset of cases.\n",
        "\n",
        "Other Columns: All other features, particularly those related to the TBP image analysis (e.g., tbp_lv_ series), have complete data with no missing values.\n",
        "\n",
        "### **Summary:**\n",
        "The dataset contains a significant amount of missing data in key demographic and diagnostic features (age_approx, sex, anatom_site_general).\n",
        "\n",
        "The high rate of missing data in specific diagnostic fields (e.g., lesion_id, iddx_2 to iddx_5) and melanoma-specific features (mel_mitotic_index, mel_thick_mm) suggests that these columns are either highly specialized or irrelevant to most cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb5a7aMVWCKg"
      },
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "train_metadata.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Y8CXKicuQL"
      },
      "source": [
        "### **Summary Statistics Observations:**\n",
        "**Target Variable (target):**\n",
        "\n",
        "Mean: The mean is approximately 0.00098, indicating that the dataset is heavily skewed toward benign cases (almost all cases are benign, with very few malignant).\n",
        "\n",
        "Std (Standard Deviation): The very low standard deviation (0.031) confirms this skewness.\n",
        "Min/Max: The binary nature of the target variable is reflected, with a minimum of 0 (benign) and a maximum of 1 (malignant).\n",
        "\n",
        "**Age Approximation (age_approx):**\n",
        "\n",
        "Mean: The average age is around 58 years, with a standard deviation of approximately 13.6 years.\n",
        "\n",
        "Range: Ages range from 5 to 85 years, with the middle 50% of patients (IQR) aged between 50 and 70 years. This suggests the dataset primarily involves older adults, who are generally at higher risk for skin cancer.\n",
        "\n",
        "**Clinical Size of Lesion (clin_size_long_diam_mm):**\n",
        "\n",
        "Mean: The average lesion size is about 3.93 mm, with a standard deviation of 1.74 mm.\n",
        "\n",
        "Range: The lesion sizes vary widely, from 1 mm to a maximum of 28.4 mm, with the majority of lesions between 2.84 mm and 4.38 mm. Larger lesions might indicate a higher risk, but the majority are relatively small.\n",
        "\n",
        "**Lesion Characteristics (tbp_lv_* columns):**\n",
        "\n",
        "These columns represent various lesion characteristics related to color, contrast, and symmetry, derived from image analysis:\n",
        "\n",
        "Mean Values: These values vary widely, with some features like tbp_lv_A and tbp_lv_B averaging around 20-28, while others like tbp_lv_H (hue) have a mean of 54.65.\n",
        "\n",
        "Range: The values range significantly, indicating that these features capture a broad spectrum of lesion characteristics, which could be crucial for distinguishing between benign and malignant lesions.\n",
        "\n",
        "**Melanoma Thickness (mel_thick_mm):**\n",
        "\n",
        "Mean: For the subset of data where this is available, the average melanoma thickness is around 0.67 mm, with a maximum thickness of 5.0 mm. This feature is important for assessing the severity of melanomas.\n",
        "\n",
        "**AI Model Confidence (tbp_lv_dnn_lesion_confidence):**\n",
        "\n",
        "Mean: The AI model’s confidence in identifying lesions is high, with an average confidence score of 97.16 out of 100.\n",
        "\n",
        "Range: Confidence ranges from almost zero to 100, showing that the model is highly confident in most of its assessments, but there are cases where it is uncertain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X6Iu-wfWL35"
      },
      "outputs": [],
      "source": [
        "# Visualize class distribution of the target variable (benign vs. malignant)\n",
        "sns.countplot(x='target', data=train_metadata)\n",
        "plt.title('Distribution of Benign vs. Malignant Cases')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdvxowNwd-hC"
      },
      "source": [
        "The class distribution visualization shows a severe imbalance, with almost all cases being benign (target = 0) and very few malignant (target = 1). This imbalance poses a risk of bias in model predictions toward the benign class. To address this, considering resampling techniques, using weighted loss functions, and focusing on metrics like ROC-AUC, precision, recall, and F1-score, particularly for the malignant class is necessary. Handling this imbalance is crucial for developing an effective model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvTnUHQWXuxo"
      },
      "source": [
        "# 3. Image Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5ou8t0pWbv1"
      },
      "outputs": [],
      "source": [
        "# Visualize a few sample images from the dataset along with their labels\n",
        "image_dir = 'isic2024/train-image/image/'\n",
        "\n",
        "def show_images(image_ids, num_images=5):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, image_id in enumerate(image_ids[:num_images]):\n",
        "        image_path = os.path.join(image_dir, image_id + '.jpg')\n",
        "        image = Image.open(image_path)\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"ID: {image_id}\")\n",
        "    plt.show()\n",
        "\n",
        "sample_ids = train_metadata['isic_id'].sample(5, random_state=42).values\n",
        "show_images(sample_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crzD-V2secwB"
      },
      "source": [
        "### **Key Observations:**\n",
        "\n",
        "**Visual Diversity:**\n",
        "\n",
        "The images show a variety of lesion appearances, with differences in size, color, texture, and surrounding skin tone. This diversity highlights the challenge of skin cancer detection, as lesions can vary widely in their presentation.\n",
        "\n",
        "**Image Quality:**\n",
        "\n",
        "The images are of good resolution and focus, which is important for accurate feature extraction during model training.\n",
        "\n",
        "**Potential Features:**\n",
        "\n",
        "Visual characteristics like lesion color, border irregularity, and contrast with surrounding skin could be crucial features for distinguishing between benign and malignant cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GJyj-euXahg"
      },
      "outputs": [],
      "source": [
        "# Check image dimensions to ensure consistency across the dataset\n",
        "image_sizes = []\n",
        "for image_id in train_metadata['isic_id'].sample(100):\n",
        "    image_path = os.path.join(image_dir, image_id + '.jpg')\n",
        "    with Image.open(image_path) as img:\n",
        "        image_sizes.append(img.size)\n",
        "image_sizes_df = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
        "sns.jointplot(x='Width', y='Height', data=image_sizes_df, kind='scatter')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw8NXif2ebrM"
      },
      "source": [
        "### **Image Dimensions Consistency:**\n",
        "**Observation:** The scatter plot of image dimensions (Width vs. Height) shows a strong linear relationship, indicating that images are mostly consistent in their aspect ratio. However, there is some variation in both width and height, suggesting that not all images are of the same size.\n",
        "\n",
        "**Next Steps:** To ensure consistency during model training, you should resize all images to a common dimension as part of the preprocessing pipeline. This will standardize the input data and improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNSjFU2DX4ZU"
      },
      "source": [
        "# 4. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h35od8ZuX2I_"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of patient ages\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_metadata['age_approx'], kde=True)\n",
        "plt.title('Age Distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgneG_LzfP9L"
      },
      "source": [
        "### **Age Distribution Analysis:**\n",
        "**Observation:** The age distribution is multimodal, with noticeable peaks at specific ages, particularly in the 50-70 age range. This indicates that the dataset predominantly consists of older adults, which aligns with the higher risk of skin cancer in this age group.\n",
        "\n",
        "**Next Steps:** Given the concentration of cases in older age groups, age should be considered a significant feature in model training. May also want to investigate any correlations between age and lesion malignancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHlRzKD3YFdF"
      },
      "outputs": [],
      "source": [
        "# Examine correlations between numerical metadata features\n",
        "# Drop non-numeric columns that are not useful for correlation\n",
        "train_metadata_numeric = train_metadata.drop(columns=['isic_id', 'patient_id'])  # Drop any non-numeric identifiers\n",
        "\n",
        "# Select only numeric columns again if needed\n",
        "numeric_columns = train_metadata_numeric.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate and visualize the correlation matrix\n",
        "corr_matrix = numeric_columns.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Numeric Metadata')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCnjm-Rfw9m"
      },
      "source": [
        "### **Correlation Matrix Analysis:**\n",
        "**Target Correlation:** The target variable (benign vs. malignant) shows weak correlations with other features, indicating that no single feature strongly predicts malignancy on its own.\n",
        "\n",
        "The correlation between age_approx and target (lesion malignancy) is approximately 0.01, as indicated in the correlation matrix. This very low correlation suggests that age alone is not a strong predictor of whether a lesion is malignant or benign in this dataset. However, age might still play an indirect role when combined with other features.\n",
        "\n",
        "**Feature Interrelationships:** Some features like tbp_lv_A, tbp_lv_B, tbp_lv_C, and their extended variants show moderate to strong correlations with each other, suggesting they capture similar aspects of the lesions (e.g., color or texture).\n",
        "\n",
        "**Multicollinearity Risk:** The presence of strong correlations between certain features (e.g., between different tbp_lv_ features) indicates potential multicollinearity, which may require dimensionality reduction techniques like PCA or careful feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFzEE-LgYQDF"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of lesion locations on the body\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y='anatom_site_general', data=train_metadata, order=train_metadata['anatom_site_general'].value_counts().index)\n",
        "plt.title('Distribution of Lesion Locations')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKjmHYfmgpXc"
      },
      "source": [
        "### **Distribution of Lesion Locations:**\n",
        "**Observation:** The majority of lesions are located on the posterior torso, followed by the lower extremity, anterior torso, and upper extremity. The head/neck region has the fewest lesions.\n",
        "\n",
        "**Implications:** The high prevalence of lesions on the torso regions (both anterior and posterior) suggests these areas might be more prone to skin conditions being studied. The model should account for location as a potentially significant feature, as different body parts might have varying risks of malignancy.\n",
        "\n",
        "**Skewed Distribution:** Given that the distribution is not uniform and is skewed towards certain locations (especially the posterior torso), it would be reasonable to fill the missing values with the most common location, which is the posterior torso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FD6ZE-gYYdM"
      },
      "source": [
        "#5. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52N8agGYfMC"
      },
      "source": [
        "Handling Missing Values:\n",
        "\n",
        "Imputation: Fill missing values in numerical columns with the median.\n",
        "Mode for categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ-EtuitYXDt"
      },
      "outputs": [],
      "source": [
        "train_metadata['age_approx'].fillna(train_metadata['age_approx'].median(), inplace=True)\n",
        "train_metadata['sex'].fillna(train_metadata['sex'].mode()[0], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXbSk6AWlEog"
      },
      "outputs": [],
      "source": [
        "# Fill missing values with the most frequent location (mode)\n",
        "train_metadata['anatom_site_general'].fillna(train_metadata['anatom_site_general'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkkwIquDYyE7"
      },
      "source": [
        "Encoding Categorical Variables:\n",
        "\n",
        "One-Hot Encoding: Convert categorical variables like sex and anatom_site_general into one-hot encoded vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N-83gUyYxFA"
      },
      "outputs": [],
      "source": [
        "train_metadata = pd.get_dummies(train_metadata, columns=['sex', 'anatom_site_general'], drop_first=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZg8La8sY5Un"
      },
      "source": [
        "### **Normalizing Continuous Variables:**\n",
        "**Improves Model Convergence:** Normalizing continuous features helps gradient-based models like neural networks converge more quickly during training.\n",
        "\n",
        "R**educes Bias Toward Larger Values:** Features with larger scales can dominate the learning process, leading to models that might underutilize features with smaller scales.\n",
        "\n",
        "**Standardization:** Normalize continuous variables like age_approx to ensure consistent input scales for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_qAw0Kkl_Sw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify continuous columns to normalize\n",
        "continuous_columns = [\n",
        "    'age_approx', 'clin_size_long_diam_mm', 'mel_thick_mm',\n",
        "    'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
        "    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext',\n",
        "    'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2',\n",
        "    'tbp_lv_perimeterMM', 'tbp_lv_color_std_mean',\n",
        "    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL',\n",
        "    'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity',\n",
        "    'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence',\n",
        "    'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
        "    'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
        "    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
        "    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_dnn_lesion_confidence'\n",
        "]\n",
        "\n",
        "# Normalize the continuous columns\n",
        "scaler = StandardScaler()\n",
        "train_metadata[continuous_columns] = scaler.fit_transform(train_metadata[continuous_columns])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoac5zpmQDn"
      },
      "source": [
        "# **Final Important Observations Before Feature Engineering:**\n",
        "## **Class Imbalance:**\n",
        "\n",
        "The dataset is heavily skewed towards benign cases, which may cause models to bias towards predicting benign outcomes. Addressing this imbalance will be crucial in feature engineering and model development.\n",
        "\n",
        "## **Lesion Location Skewness:**\n",
        "\n",
        "The distribution of lesion locations is skewed, with a majority of lesions found on the posterior torso.  Missing values for lesion location with the mode (posterior torso) are filled, but this skewness should be kept in mind during feature engineering to avoid bias.\n",
        "\n",
        "## **Weak Correlations with Malignancy:**\n",
        "\n",
        "Individual features generally show weak correlations with the target variable (malignancy). This suggests that single features may not be strong predictors, and the model will likely benefit from interactions and combinations of multiple features.\n",
        "\n",
        "##**Missing Values Handled:**\n",
        "\n",
        "Missing values in critical columns like age_approx, sex, and anatom_site_general have been filled appropriately, ensuring that the dataset is complete and ready for modeling.\n",
        "\n",
        "##**Feature Scaling:**\n",
        "\n",
        "Continuous features, including age_approx, lesion sizes, and various lesion characteristics, have been normalized. This ensures that all features are on a comparable scale, which is particularly important for models sensitive to feature magnitude, such as neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHt0V2rqnrsq"
      },
      "source": [
        "# **Considerations for Feature Engineering:**\n",
        "\n",
        "1.   Addressing Class Imbalance: To balance the dataset and prevent models from being biased towards the majority class.\n",
        "2.   Explore creating interaction features (e.g., combinations of age, lesion size, and location) to capture more complex patterns in the data.\n",
        "3.   Dimensionality Reduction: to reduce multicollinearity and noise while retaining important information.\n",
        "4.  Feature Selection: With many features available, it is important to identify which ones are most predictive of malignancy.\n",
        "5. Feature Transformation: Consider transforming certain features if their distributions are highly skewed. This can make patterns in the data more linear and easier for models to learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1l_tqZpTLd"
      },
      "source": [
        "# **6. Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CFgCRFb9eB"
      },
      "source": [
        "##### As per the pre-processing data, we can see some columns do not contribute much to the analysis. Those columns are very specific to some cases, hence we drop such columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZYS3dgFItZm"
      },
      "outputs": [],
      "source": [
        "train_metadata.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ACDh3hxnQST"
      },
      "outputs": [],
      "source": [
        "#The below columns do not add much significance in predicting hence these columns are dropped\n",
        "columns_to_drop = ['image_type', 'tbp_lv_location_simple','copyright_license','lesion_id','iddx_full','iddx_1',\n",
        "                    'iddx_2','iddx_3','iddx_4','iddx_5',\n",
        "                    'mel_mitotic_index','mel_thick_mm','patient_id']\n",
        "train_metadata = train_metadata.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL8t4Hghcp6f"
      },
      "source": [
        "## **Creating new features:**\n",
        "\n",
        "The individual features in this dataset, like tbp_lv_areaMM2, tbp_lv_perimeterMM have high correlation to each other but not towards our target variable when checked independently. Hence some of these features are combined together to create a new feature.\n",
        "\n",
        "These new features will be integrated to our existing training dataframe and will be further analysed by performing correlation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfbIhV1r5WSP"
      },
      "outputs": [],
      "source": [
        "# 1. Interactions Involving Lesion Size and Shape\n",
        "train_metadata['aspect_ratio_interaction'] = train_metadata['clin_size_long_diam_mm'] / train_metadata['tbp_lv_minorAxisMM']\n",
        "train_metadata['size_perimeter_interaction'] = train_metadata['clin_size_long_diam_mm'] * train_metadata['tbp_lv_perimeterMM']\n",
        "train_metadata['area_perimeter_interaction'] = train_metadata['tbp_lv_areaMM2'] * train_metadata['tbp_lv_perimeterMM']\n",
        "\n",
        "# 2. Color and Texture Interactions\n",
        "train_metadata['color_variability_deltaA_interaction'] = train_metadata['tbp_lv_color_std_mean'] * train_metadata['tbp_lv_deltaA']\n",
        "train_metadata['color_variability_deltaB_interaction'] = train_metadata['tbp_lv_color_std_mean'] * train_metadata['tbp_lv_deltaB']\n",
        "train_metadata['color_variability_deltaL_interaction'] = train_metadata['tbp_lv_color_std_mean'] * train_metadata['tbp_lv_deltaL']\n",
        "train_metadata['color_asymmetry_symmetry_interaction'] = train_metadata['tbp_lv_radial_color_std_max'] * train_metadata['tbp_lv_symm_2axis']\n",
        "\n",
        "# 3. Geometric and Positional Interactions\n",
        "train_metadata['symmetry_eccentricity_interaction'] = train_metadata['tbp_lv_symm_2axis'] * train_metadata['tbp_lv_eccentricity']\n",
        "train_metadata['size_x_coord_interaction'] = train_metadata['clin_size_long_diam_mm'] * train_metadata['tbp_lv_x']\n",
        "\n",
        "# 4. Clinical Estimations and Standard Measurements Interactions\n",
        "train_metadata['nevi_confidence_border_irregularity_interaction'] = train_metadata['tbp_lv_nevi_confidence'] * train_metadata['tbp_lv_norm_border']\n",
        "train_metadata['color_border_irregularity_interaction'] = train_metadata['tbp_lv_norm_color'] * train_metadata['tbp_lv_norm_border']\n",
        "\n",
        "# 5. Advanced Interactions Involving Age\n",
        "train_metadata['age_nevi_confidence_interaction'] = train_metadata['age_approx'] * train_metadata['tbp_lv_nevi_confidence']\n",
        "train_metadata['age_symmetry_interaction'] = train_metadata['age_approx'] * train_metadata['tbp_lv_symm_2axis']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhco4U1H5-UM"
      },
      "outputs": [],
      "source": [
        "train_metadata.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6b4CxCHtECI"
      },
      "outputs": [],
      "source": [
        "train_metadata_copy = train_metadata.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L539aOrQIfjo"
      },
      "outputs": [],
      "source": [
        "# List of newly created interaction features\n",
        "interaction_features = [\n",
        "    'aspect_ratio_interaction',\n",
        "    'size_perimeter_interaction',\n",
        "    'area_perimeter_interaction',\n",
        "    'color_variability_deltaA_interaction',\n",
        "    'color_variability_deltaB_interaction',\n",
        "    'color_variability_deltaL_interaction',\n",
        "    'color_asymmetry_symmetry_interaction',\n",
        "    'symmetry_eccentricity_interaction',\n",
        "    'size_x_coord_interaction',\n",
        "    'nevi_confidence_border_irregularity_interaction',\n",
        "    'color_border_irregularity_interaction',\n",
        "    'age_nevi_confidence_interaction',\n",
        "    'age_symmetry_interaction'\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB9urAAL2uZ3"
      },
      "source": [
        "##### Performing the correlation between the newly created features and old features with our target feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B8wuMAzNVr"
      },
      "outputs": [],
      "source": [
        "# Drop any non-numeric identifiers\n",
        "train_metadata_features = train_metadata.drop(columns=['isic_id'])\n",
        "\n",
        "# Select only numeric columns again if needed\n",
        "numeric_columns = train_metadata_features.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate and visualize the correlation matrix\n",
        "corr_matrix = numeric_columns.corr()\n",
        "target_correlations = corr_matrix['target'].sort_values(ascending=False)\n",
        "\n",
        "# Print the correlation values\n",
        "print(target_correlations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M93U7tF81RW8"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "\n",
        "##### Since the dataset has numerous amount of features, we can perform dimensionality reduction to understand the most important features which retains maximum information.\n",
        "\n",
        "##### We have 3 different categories of data: boolean, categorical (object type) and numeric. Since PCA expects only the numeric kind of data, we adjust the data accordingly to perform PCA.\n",
        "\n",
        "As of now the principal components are kept at 25. This can be tuned depending on the model performance as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yd3meyAUh_P"
      },
      "outputs": [],
      "source": [
        "#Performing encoding of the location column of the lesion (tbp_lv_location)\n",
        "location_encoding = pd.get_dummies(train_metadata, columns=['tbp_lv_location'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PPhoC-aXkBW"
      },
      "outputs": [],
      "source": [
        "train_metadata = location_encoding\n",
        "train_metadata.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "993iiMb9R3qC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Separate `isic_id` as it is a unique id for each row\n",
        "isic_id = train_metadata['isic_id']\n",
        "target = train_metadata['target']\n",
        "train_metadata = train_metadata.drop(['target'], axis=1)  # Remove target before PCA\n",
        "\n",
        "# # Define preprocessing for categorical features (object types)\n",
        "# categorical_features = train_metadata.select_dtypes(include=['object']).columns.tolist()\n",
        "# categorical_transformer = Pipeline(steps=[\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('onehot', OneHotEncoder(drop='first'))\n",
        "# ])\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_features = train_metadata.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for boolean features\n",
        "boolean_features = train_metadata.select_dtypes(include=['bool']).columns.tolist()\n",
        "boolean_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Combining preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('bool', boolean_transformer, boolean_features)\n",
        "    ])\n",
        "\n",
        "# Creating the pipeline with preprocessing and PCA\n",
        "pca_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('pca', PCA(n_components=25))])  # Adjust `n_components` as needed\n",
        "\n",
        "# Apply the pipeline to the data\n",
        "pca_result = pca_pipeline.fit_transform(train_metadata)\n",
        "\n",
        "# Re-attach `isic_id` to the PCA results\n",
        "train_metadata_pca = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])\n",
        "train_metadata_pca['isic_id'] = isic_id.values\n",
        "\n",
        "# The `train_metadata_pca` DataFrame now contains the PCA results along with the `isic_id` for each observation\n",
        "print(train_metadata_pca.head())\n",
        "\n",
        "# Optionally, if you want to add the target back for further processing or model building:\n",
        "train_metadata_pca['target'] = target.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neDTE9u8vt0e"
      },
      "outputs": [],
      "source": [
        "# Explained Variance Ratio by each Principal Component (PC)\n",
        "explained_variance = pca_pipeline.named_steps['pca'].explained_variance_ratio_\n",
        "print(\"Explained Variance by each PC:\\n\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")\n",
        "\n",
        "# Feature Names after Preprocessing\n",
        "feature_names = numeric_features + boolean_features\n",
        "\n",
        "# PCA Components (Loadings)\n",
        "components = pca_pipeline.named_steps['pca'].components_\n",
        "\n",
        "#DataFrame to show the loadings\n",
        "loadings_df = pd.DataFrame(components.T, columns=[f'PC{i+1}' for i in range(components.shape[0])], index=feature_names)\n",
        "\n",
        "\n",
        "#To see the loadings and explained variance together:\n",
        "print(\"\\nExplained Variance and Top Contributing Features per PC:\\n\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"PC{i+1}: {var:.4f} variance explained\")\n",
        "    print(\"Top contributing features:\")\n",
        "    top_features = loadings_df.iloc[:, i].abs().sort_values(ascending=False).head(5)\n",
        "    print(top_features)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHK3eOCxA9Gw"
      },
      "source": [
        "#### Creating dataset for model development using the selected features from the PCA and correlation analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_metadata_pca.columns"
      ],
      "metadata": {
        "id": "770QBJv8QZm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxPEnFa07ikU"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        "    'isic_id',\n",
        "    'PC2',\n",
        "    'PC3',\n",
        "    'PC1',\n",
        "    'PC4',\n",
        "    'PC5',\n",
        "    'target'\n",
        "]\n",
        "\n",
        "# Attach `isic_id` to the train_metadata set as it was earlier dropped\n",
        "# train_metadata['isic_id'] = isic_id.values\n",
        "train_metadata_selected_features = train_metadata_pca[selected_features]\n",
        "# train_metadata_selected_features['target'] = train_metadata['target']\n",
        "print(train_metadata_selected_features.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48f1iYIE80CN"
      },
      "outputs": [],
      "source": [
        "#Final dataset which has to be used for the model development\n",
        "train_metadata_selected_features.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_xeGCv1JExr"
      },
      "source": [
        "# Resampling Techniques to Help Class Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0ofkx-JOWH"
      },
      "source": [
        "### Oversampling the Minority Class (Malignant Cases):\n",
        "SMOTE (Synthetic Minority Over-sampling Technique): This technique generates synthetic samples in the feature space for the minority class to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSwB-zZeI22m"
      },
      "outputs": [],
      "source": [
        "# from imblearn.over_sampling import SMOTE\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Separate features and target\n",
        "# X = train_metadata_selected_features.drop(columns=['target'])\n",
        "# y = train_metadata_selected_features['target']\n",
        "\n",
        "# # Check if 'isic_id' exists in the DataFrame\n",
        "# if 'isic_id' in X.columns:\n",
        "#     isic_id = X['isic_id']\n",
        "#     X = X.drop(columns=['isic_id'])  # Drop 'isic_id' if it exists\n",
        "# else:\n",
        "#     isic_id = None\n",
        "\n",
        "# # Split the data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# # Apply SMOTE to the training data\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# # Optionally, track synthetic samples or reattach 'isic_id'\n",
        "# X_train_smote = pd.DataFrame(X_train_smote, columns=X_train.columns)\n",
        "\n",
        "# # Handle 'isic_id' for synthetic samples (if applicable)\n",
        "# if isic_id is not None:\n",
        "#     # Generate synthetic 'isic_id'\n",
        "#     new_isic_ids = ['synthetic_' + str(i) for i in range(len(X_train_smote) - len(X_train))]\n",
        "#     original_isic_ids = isic_id.iloc[X_train.index].tolist()  # Get 'isic_id' for original training samples\n",
        "#     X_train_smote['isic_id'] = original_isic_ids + new_isic_ids\n",
        "# else:\n",
        "#     print(\"'isic_id' was not found in the original data.\")\n",
        "\n",
        "# # For validation set, reattach original 'isic_id' if it exists\n",
        "# if isic_id is not None:\n",
        "#     X_val['isic_id'] = isic_id.iloc[X_val.index].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14W0DxK0gbLM"
      },
      "outputs": [],
      "source": [
        "# X_train_smote.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDnQc6MiJUnE"
      },
      "source": [
        " After applying SMOTE, we have to ensure to validate the model using cross-validation or on a separate validation set to check if the model’s performance on the minority class has improved without causing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing LightGBM to handle the class imbalance and perform cross-validation.\n",
        "\n",
        "LightBGM has built-in support for handling imbalanced data via parameters like 'scale_pos_weight'."
      ],
      "metadata": {
        "id": "dfyKy8PZz_Ag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tVjOGCkh-jk"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "# import lightgbm as lgb\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# num_negatives = sum(y_train_smote == 0)\n",
        "# num_positives = sum(y_train_smote == 1)\n",
        "# scale_pos_weight = num_negatives / num_positives\n",
        "# print(f'Scale Pos Weight: {scale_pos_weight}')\n",
        "\n",
        "\n",
        "# X_train_smote = X_train_smote.drop(columns=['isic_id'])\n",
        "# X_val = X_val.drop(columns=['isic_id'])\n",
        "\n",
        "\n",
        "# scale_pos_weight = num_negatives / num_positives\n",
        "\n",
        "# # Initialize and train the LightGBM model\n",
        "# lgb_model = lgb.LGBMClassifier(random_state=42, scale_pos_weight=scale_pos_weight)\n",
        "# lgb_model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# # Predict on the validation set with LightGBM\n",
        "# y_val_pred_lgb = lgb_model.predict(X_val)\n",
        "\n",
        "# # Print the confusion matrix and classification report for LightGBM\n",
        "# print(\"\\nConfusion Matrix on Validation Set (LightGBM):\")\n",
        "# print(confusion_matrix(y_val, y_val_pred_lgb))\n",
        "# print(\"\\nClassification Report on Validation Set (LightGBM):\")\n",
        "# print(classification_report(y_val, y_val_pred_lgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis of the confusion matrix**\n",
        "\n",
        "*   **Class 0 (Majority Class)**: The model correctly identified 93% of the actual class 0 cases as seen by the recall score\n",
        "*   **Class 1 (Minority Class)**: The model identified 63% of the actual class 1 cases. The F1-score is very low, indicating poor balance between precision and recall for class 1.\n",
        "\n",
        "\n",
        "*   **Weighted Avg**: These metrics are heavily skewed by the majority class, leading to a high overall score but masking the performance on the minority class.\n",
        "*   The recall for class 1 (minority class) is 0.63, which is a good result given the class imbalance. However, the precision remains very low at 0.01, indicating that many instances are incorrectly classified as positive.\n",
        "\n",
        "The model is performing well on the majority class but is still struggling with precision for the minority class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sf6ugmdAngov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAxfLhAZqWfy"
      },
      "outputs": [],
      "source": [
        "# # Performing cross-validation on training set\n",
        "\n",
        "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# # Define the cross-validation strategy\n",
        "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# # X_train_smote = X_train_smote.drop(columns=['isic_id'])\n",
        "\n",
        "# # Perform cross-validation\n",
        "# cv_scores = cross_val_score(lgb_model, X_train_smote, y_train_smote, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
        "\n",
        "# # Evaluate and print results\n",
        "# print(f'Cross-Validation F1 Macro Scores: {cv_scores}')\n",
        "# print(f'Mean CV F1 Macro Score: {cv_scores.mean()}')\n",
        "# print(f'Standard Deviation of CV Scores: {cv_scores.std()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import h5py\n",
        "from PIL import Image\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yps4dWBKJkc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('isic2024/train-image.hdf5', 'r') as file:\n",
        "    print(list(file.keys()))  # Check all available keys\n",
        "    data = file['ISIC_0015845']\n",
        "    print(data.shape, data.dtype)\n"
      ],
      "metadata": {
        "id": "jHhdq0smRCO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "\n",
        "def decode_image_from_compressed_string(byte_string):\n",
        "    \"\"\" Decode a compressed image from a byte string. \"\"\"\n",
        "    image_stream = io.BytesIO(byte_string)\n",
        "    image = Image.open(image_stream)\n",
        "    return image\n",
        "\n",
        "class SkinCancerDataset(Dataset):\n",
        "    def __init__(self, metadata, image_file, mode='train', transform=None):\n",
        "        self.metadata = metadata\n",
        "        self.image_file = image_file\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        isic_id = self.metadata.iloc[idx]['isic_id']\n",
        "        with h5py.File(self.image_file, 'r') as f:\n",
        "            if isic_id in f:\n",
        "                byte_string = f[isic_id][()].tobytes()  # Read as bytes\n",
        "                try:\n",
        "                    image = decode_image_from_compressed_string(byte_string)\n",
        "                except IOError:\n",
        "                    raise ValueError(\"Failed to decode image\")\n",
        "            else:\n",
        "                raise KeyError(f\"ISIC ID {isic_id} not found in HDF5 file.\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        metadata_features = self.metadata.iloc[idx][1:-1].values.astype(np.float32)  # Exclude 'isic_id' and 'target'\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            return image, metadata_features, isic_id\n",
        "        else:\n",
        "            label = self.metadata.iloc[idx]['target']\n",
        "            return image, metadata_features, label\n",
        "\n",
        "# transform and dataset usage\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load and split the data\n",
        "train_data, val_data = train_test_split(train_metadata_selected_features, test_size=0.2, random_state=42)\n",
        "test_data = pd.read_csv('isic2024/test-metadata.csv')\n",
        "\n",
        "train_dataset = SkinCancerDataset(train_data, 'isic2024/train-image.hdf5', 'train', transform)\n",
        "val_dataset = SkinCancerDataset(val_data, 'isic2024/train-image.hdf5', 'train', transform)\n",
        "test_dataset = SkinCancerDataset(test_data, 'isic2024/test-image.hdf5', 'test', transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cgqOtssLQVtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "\n",
        "# Model definition\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, num_metadata_features):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.cnn = models.densenet121(pretrained=True)\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        self.fc1 = nn.Linear(1024 + num_metadata_features, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, image, metadata):\n",
        "        image_features = self.cnn(image)\n",
        "        combined_features = torch.cat((image_features, metadata), dim=1)\n",
        "        x = self.fc1(combined_features)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# Prepare the model\n",
        "num_metadata_features = len(train_data.columns) - 2  # excluding isic_id and target\n",
        "model = CombinedModel(num_metadata_features)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "6XTVmzCYUpUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=True)\n",
        "        for images, metadata, labels in train_iterator:\n",
        "            images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, metadata)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_iterator.set_postfix({'Train Loss': running_loss / len(train_loader)})\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        model.eval()\n",
        "        val_iterator = tqdm(val_loader, desc='Validation', leave=True)\n",
        "        with torch.no_grad():\n",
        "            for images, metadata, labels in val_iterator:\n",
        "                images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
        "                outputs = model(images, metadata)\n",
        "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "                val_iterator.set_postfix({'Val Loss': val_loss / val_steps})\n",
        "\n",
        "        print(f'Epoch {epoch+1} Train Loss: {running_loss / len(train_loader):.4f} Val Loss: {val_loss / val_steps:.4f}')\n"
      ],
      "metadata": {
        "id": "PHUgSlQqXEiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_set(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    isic_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, metadata, ids in test_loader:\n",
        "            images, metadata = images.to(device), metadata.to(device)\n",
        "            preds = model(images, metadata).squeeze()\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            isic_ids.extend(ids)\n",
        "\n",
        "    return isic_ids, predictions\n",
        "\n",
        "def create_submission_file(isic_ids, predictions, filename=\"submission.csv\"):\n",
        "    submission_df = pd.DataFrame({\n",
        "        \"isic_id\": isic_ids,\n",
        "        \"target\": predictions\n",
        "    })\n",
        "    submission_df.to_csv(filename, index=False)\n",
        "    print(f\"Submission file {filename} created.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3CD06yJaZIpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, metadata, labels in val_loader:\n",
        "            images, metadata, labels = images.to(device), metadata.to(device), labels.to(device)\n",
        "            preds = model(images, metadata).squeeze()\n",
        "            targets.extend(labels.tolist())\n",
        "            outputs.extend(preds.cpu().tolist())\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(targets, outputs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Calculate pAUC for TPR >= 0.8\n",
        "    start_index = next(i for i, x in enumerate(tpr) if x >= 0.8)\n",
        "    pAUC = auc(fpr[start_index:], tpr[start_index:])\n",
        "\n",
        "    print(f\"Overall AUC: {roc_auc:.4f}\")\n",
        "    print(f\"pAUC (TPR >= 0.8): {pAUC:.4f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc, pAUC\n"
      ],
      "metadata": {
        "id": "yAjR6pBsJrJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, val_loader, 10)\n"
      ],
      "metadata": {
        "id": "IkVr_bQ9ZSg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')\n"
      ],
      "metadata": {
        "id": "BHeaY_Q2m_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc, pAUC = evaluate_model(model, val_loader)\n",
        "isic_ids, predictions = predict_test_set(model, test_loader)\n",
        "create_submission_file(isic_ids, predictions)"
      ],
      "metadata": {
        "id": "oAHy2ql1hXpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}